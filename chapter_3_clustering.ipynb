{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of Words** -  order of words is ignored and word counts collected.\n",
    "1.  Extract salient features from each post and store it as a vector per post\n",
    "2.  Compute clustering on the vectors\n",
    "3.  From this cluster, fetch a handful of posts having a similarity to the post in question. (diversity)\n",
    "\n",
    "#Preprocessing\n",
    "###Converting raw text into a bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['disk', 'format', 'hard', 'how', 'my', 'problems', 'to']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = [\"How to format my hard disk\", \"Hard disk format problems\"]\n",
    "X = vectorizer.fit_transform(content)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Counting Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post Filename</th>\n",
       "      <th>Post Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01.txt</td>\n",
       "      <td>This is a toy post about machine learning. Act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02.txt</td>\n",
       "      <td>Imaging databases provide storage capabilities.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03.txt</td>\n",
       "      <td>Most imaging databases save images permanently.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04.txt</td>\n",
       "      <td>Imaging databases store data.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05.txt</td>\n",
       "      <td>Imaging databases store data. Imaging database...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Post Filename                                       Post Content\n",
       "0        01.txt  This is a toy post about machine learning. Act...\n",
       "1        02.txt    Imaging databases provide storage capabilities.\n",
       "2        03.txt  Most imaging databases save images permanently.\\n\n",
       "3        04.txt                      Imaging databases store data.\n",
       "4        05.txt  Imaging databases store data. Imaging database..."
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def read_directory_to_list(directory='./repo/ch03/data/toy'):\n",
    "    import os\n",
    "    result = pd.DataFrame(columns=['Post Filename','Post Content'])\n",
    "    for fn in os.listdir(directory):\n",
    "        filename = directory+'/'+fn\n",
    "        with open(filename, 'r') as f:\n",
    "            result = result.append({'Post Filename': fn,\n",
    "                                    'Post Content': f.read()},\n",
    "                          ignore_index=True)\n",
    "    return result\n",
    "posts = read_directory_to_list()\n",
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "#We have to notify the vectorizer about the full dataset up front\n",
    "#so that it knows what words are to be expected.\n",
    "posts = posts['Post Content']\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"#samples: %d, #features: %d\"%(num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returns sparse array\n",
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n",
      "\n",
      "to array:\n",
      "[[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#We can now vectorize our new post.\n",
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post]) #note [list]\n",
    "\n",
    "#returns a sparse matrix\n",
    "print(\"Returns sparse array\")\n",
    "print(new_post_vec)\n",
    "print()\n",
    "\n",
    "new_post_vec = new_post_vec.toarray()\n",
    "print('to array:')\n",
    "print(new_post_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Naive Version - Euclidean distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New post: imaging databases\n",
      "\n",
      "=== Post 0 with dist=4.00: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=1.73: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=2.00: Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist=1.41: Imaging databases store data.\n",
      "=== Post 4 with dist=5.10: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist=1.41\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1-v2\n",
    "    return sp.linalg.norm(delta) #calculates euclidean difference\n",
    "\n",
    "def find_euclidean_closest(new_post, dist_func, X_train=X_train):\n",
    "    print(\"New post: %s\"%(new_post))\n",
    "    print()\n",
    "    new_post_vec = vectorizer.transform([new_post])\n",
    "    new_post_vec = new_post_vec.toarray()\n",
    "    \n",
    "    best_doc = None\n",
    "    best_dist = sp.inf\n",
    "    best_i = None\n",
    "    for i, post in enumerate(posts):\n",
    "        if post == new_post:\n",
    "            continue\n",
    "        post_vec = X_train.getrow(i).toarray()\n",
    "        d = dist_func(post_vec, new_post_vec)\n",
    "        print(\"=== Post %i with dist=%.2f: %s\"%(i,d,post))\n",
    "        if d<best_dist:\n",
    "            best_dist = d\n",
    "            best_i = i\n",
    "\n",
    "    print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))\n",
    "    \n",
    "find_euclidean_closest(\"imaging databases\", dist_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But...check out post 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
      "[[0 0 0 0 3 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]]\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train.getrow(3).toarray())\n",
    "print(X_train.getrow(4).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Normalizing Word Count Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New post: imaging databases\n",
      "\n",
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.92: Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist=0.77\n"
     ]
    }
   ],
   "source": [
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1/sp.linalg.norm(v1)\n",
    "    v2_normalized = v2/sp.linalg.norm(v2)\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta)\n",
    "\n",
    "find_euclidean_closest(\"imaging databases\", dist_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Removing Less Important Words\n",
    "**stop words** words like \"most\" that appear in all sorts of different contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anyone', 'myself', 'cannot', 'my', 'off', 'via', 'also', 'un', 'even', 'sometime', 'whither', 'ever', 'between', 'wherein', 'everyone', 'some', 'someone', 'toward', 'ltd', 'through']\n",
      "318\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "print(list(vectorizer.get_stop_words())[:20])\n",
    "print(len(list(vectorizer.get_stop_words())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 18\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"#samples: %d, #features: %d\"%(num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New post: imaging databases\n",
      "\n",
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.86: Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist=0.77\n"
     ]
    }
   ],
   "source": [
    "find_euclidean_closest(\"imaging databases\", dist_norm, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that post 2 is now on par with post 1, whereas before removing the stop words, it was up at .94\n",
    "\n",
    "##Stemming\n",
    "We currently counts similar words in different variants as different words.  E.g. \"imaging\" and \"images.\"  **Stemming** reuces words to their specific word stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphic\n",
      "imag\n",
      "imag\n",
      "imagin\n",
      "imagin\n",
      "buy\n",
      "buy\n",
      "bought\n"
     ]
    }
   ],
   "source": [
    "import nltk.stem\n",
    "\n",
    "s = nltk.stem.SnowballStemmer('english')\n",
    "print(s.stem('graphics'))\n",
    "print(s.stem('imaging'))\n",
    "print(s.stem('image'))\n",
    "print(s.stem('imagination'))\n",
    "print(s.stem('imagine'))\n",
    "print(s.stem('buys'))\n",
    "print(s.stem('buying'))\n",
    "print(s.stem('bought'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Extending the vectorizer with NLTK's stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "vectorizer = StemmedCountVectorizer(min_df=1, stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 17\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"#samples: %d, #features: %d\"%(num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New post: imaging databases\n",
      "\n",
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.63: Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 2 with dist=0.63\n"
     ]
    }
   ],
   "source": [
    "find_euclidean_closest(\"imaging databases\", dist_norm, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the stemmed vectorizer collapses imaging and images, post 2 becomes the most similar because \"imag\" appears twice.\n",
    "\n",
    "###Stop words on Steroids\n",
    "\n",
    "*  `max_df` parameter can be set to fraction, e.g. .9, so that any word that occurs in more than 90 percent of of all posts will be ignored. But how low to set it?\n",
    "\n",
    "* **Term frequency - inverse document frequency (TF_IDF)** - counting term frequencies for every post and discount those that appear in many posts.  We want a high value for a given term in a given post if that term occurs often in that post and very seldom anywhere else.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.270310072072\n",
      "\n",
      "0.135155036036\n",
      "0.366204096223\n"
     ]
    }
   ],
   "source": [
    "#Naive implementation\n",
    "import scipy as sp\n",
    "def tfidf(term, doc, corpus):\n",
    "    #normalize so longer docs don't have advantage:\n",
    "    tf = doc.count(term) / len(doc) \n",
    "    num_docs_with_term = len([d for d in corpus if term in d])\n",
    "    idf = sp.log(len(corpus) / num_docs_with_term)\n",
    "    return tf * idf\n",
    "\n",
    "a, abb, abc = ['a'], ['a','b','b'], ['a','b','c']\n",
    "D = [a, abb, abc]\n",
    "print(\"a:\")\n",
    "print(tfidf(\"a\", a, D))\n",
    "print(tfidf(\"a\", abb, D))\n",
    "print(tfidf(\"a\", abc, D))\n",
    "print()\n",
    "print(tfidf(\"b\", abb, D))\n",
    "print()\n",
    "print(tfidf(\"b\", abc, D))\n",
    "print(tfidf(\"c\", abc, D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciKit implements this with TfidfVectorizer, which is inherited from CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (\n",
    "            english_stemmer.stem(w) for w in analyzer(doc))\n",
    "vectorizer = StemmedTfidfVectorizer(min_df=1,\n",
    "                stop_words='english', decode_error='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 17\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"#samples: %d, #features: %d\"%(num_samples, num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###So far\n",
    "1. Tokenize the text\n",
    "    * lowercase the text\n",
    "    * Throw away words that only occur once (min_df=1)\n",
    "    * Throw away stop words\n",
    "    * Collapse words onto their stems\n",
    "2. Toss words that occur way too often to be of any help detcting related posts\n",
    "3. Toss words that occur so seldom that they probably won't appear again.\n",
    "4. Count Remaining words\n",
    "5. Calculate TF-IDF values from counts, considering the whole text corpus\n",
    "\n",
    "###Limits:\n",
    "* **Does not cover word relations** \"Car hits wall\" and \"Wall hits car\" will have same feature vector\n",
    "* **Does not capture negations**.  This can be fixed by also counting \"bigrams\" and \"trigrams\" in addition to \"unigrams\"\n",
    "* **Cannot account for misspelled words**\n",
    "\n",
    "#Clustering\n",
    "* flat - i.e. key means.  Divides posts into a set of clusters without relating the clusters to each other.  Partitioning.  Needs up front number of clusters.\n",
    "* hierarchical - clusters get clustered recursively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "all_data = sklearn.datasets.fetch_20newsgroups(subset='all')\n",
    "print(len(all_data.filenames))\n",
    "all_data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3529\n"
     ]
    }
   ],
   "source": [
    "groups = ['comp.graphics', 'comp.os.ms-windows.misc',\n",
    "          'comp.sys.ibm.pc.hardware','comp.sys.mac.hardware',\n",
    "          'comp.windows.x','sci.space']\n",
    "train_data = sklearn.datasets.fetch_20newsgroups(\n",
    "        subset='train', categories=groups)\n",
    "print(len(train_data.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2349\n"
     ]
    }
   ],
   "source": [
    "test_data = sklearn.datasets.fetch_20newsgroups(\n",
    "        subset='test', categories=groups)\n",
    "print(len(test_data.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 3529, #features: 4712\n"
     ]
    }
   ],
   "source": [
    "vectorizer = StemmedTfidfVectorizer(\n",
    "        min_df=10, max_df=0.5,\n",
    "        stop_words='english',\n",
    "        decode_error='ignore')\n",
    "vectorized = vectorizer.fit_transform(train_data.data)\n",
    "num_samples, num_features = vectorized.shape\n",
    "print(\"#samples: %d, #features: %d\"% (num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 5899.560\n",
      "Iteration  1, inertia 3218.298\n",
      "Iteration  2, inertia 3184.333\n",
      "Iteration  3, inertia 3164.867\n",
      "Iteration  4, inertia 3152.004\n",
      "Iteration  5, inertia 3143.111\n",
      "Iteration  6, inertia 3136.256\n",
      "Iteration  7, inertia 3129.325\n",
      "Iteration  8, inertia 3124.567\n",
      "Iteration  9, inertia 3121.900\n",
      "Iteration 10, inertia 3120.210\n",
      "Iteration 11, inertia 3118.627\n",
      "Iteration 12, inertia 3117.363\n",
      "Iteration 13, inertia 3116.811\n",
      "Iteration 14, inertia 3116.588\n",
      "Iteration 15, inertia 3116.417\n",
      "Iteration 16, inertia 3115.760\n",
      "Iteration 17, inertia 3115.374\n",
      "Iteration 18, inertia 3115.155\n",
      "Iteration 19, inertia 3114.949\n",
      "Iteration 20, inertia 3114.515\n",
      "Iteration 21, inertia 3113.937\n",
      "Iteration 22, inertia 3113.720\n",
      "Iteration 23, inertia 3113.548\n",
      "Iteration 24, inertia 3113.475\n",
      "Iteration 25, inertia 3113.447\n",
      "Converged at iteration 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='random', max_iter=300, n_clusters=50, n_init=1,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=3, tol=0.0001,\n",
       "    verbose=1)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusters = 50\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=num_clusters, init='random', \n",
    "            n_init=1, verbose=1, random_state=3)\n",
    "km.fit(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38 17 47 ..., 41 14 16]\n",
      "(3529,)\n"
     ]
    }
   ],
   "source": [
    "# each cluster has been assigned a label\n",
    "print(km.labels_)\n",
    "print(km.labels_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n"
     ]
    }
   ],
   "source": [
    "new_post = \"\"\"\n",
    "Disk drive problems.  Hi, I have a problem with my hard disk.\n",
    "After 1 year it is working only sporadically now.\n",
    "I tried to format it, but now it doessn't boot any more.\n",
    "Any ideas?  Thanks.\n",
    "\"\"\"\n",
    "\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "new_post_label = km.predict(new_post_vec)[0]\n",
    "\n",
    "#Now that we have a clustering and a label...\n",
    "# `nonzero` makes an array of indices of nonzero numbers\n",
    "similar_indices = (km.labels_==new_post_label).nonzero()[0]\n",
    "\n",
    "similar = []\n",
    "for i in similar_indices:\n",
    "    dist = sp.linalg.norm(\n",
    "            new_post_vec - vectorized[i].toarray())\n",
    "    similar.append((dist, train_data.data[i]))\n",
    "similar = sorted(similar)\n",
    "print(len(similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0236597431380716,\n",
       " \"From: Thomas Dachsel <GERTHD@mvs.sas.com>\\nSubject: BOOT PROBLEM with IDE controller\\nNntp-Posting-Host: sdcmvs.mvs.sas.com\\nOrganization: SAS Institute Inc.\\nLines: 25\\n\\nHi,\\nI've got a Multi I/O card (IDE controller + serial/parallel\\ninterface) and two floppy drives (5 1/4, 3 1/2) and a\\nQuantum ProDrive 80AT connected to it.\\nI was able to format the hard disk, but I could not boot from\\nit. I can boot from drive A: (which disk drive does not matter)\\nbut if I remove the disk from drive A and press the reset switch,\\nthe LED of drive A: continues to glow, and the hard disk is\\nnot accessed at all.\\nI guess this must be a problem of either the Multi I/o card\\nor floppy disk drive settings (jumper configuration?)\\nDoes someone have any hint what could be the reason for it.\\nPlease reply by email to GERTHD@MVS.SAS.COM\\nThanks,\\nThomas\\n+-------------------------------------------------------------------+\\n| Thomas Dachsel                                                    |\\n| Internet: GERTHD@MVS.SAS.COM                                      |\\n| Fidonet:  Thomas_Dachsel@camel.fido.de (2:247/40)                 |\\n| Subnet:   dachsel@rnivh.rni.sub.org (UUCP in Germany, now active) |\\n| Phone:    +49 6221 4150 (work), +49 6203 12274 (home)             |\\n| Fax:      +49 6221 415101                                         |\\n| Snail:    SAS Institute GmbH, P.O.Box 105307, D-W-6900 Heidelberg |\\n| Tagline:  One bad sector can ruin a whole day...                  |\\n+-------------------------------------------------------------------+\\n\")"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_at_1 = similar[0]\n",
    "show_at_2 = similar[int(len(similar)/10)]\n",
    "show_at_3 = similar[int(len(similar)/ 2)]\n",
    "show_at_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Another look at noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(245, 'From: SITUNAYA@IBM3090.BHAM.AC.UK\\nSubject: test....(sorry)\\nOrganization: The University of Birmingham, United Kingdom\\nLines: 1\\nNNTP-Posting-Host: ibm3090.bham.ac.uk\\n\\n==============================================================================\\n', 'comp.graphics')\n"
     ]
    }
   ],
   "source": [
    "post_group = zip(train_data.data, train_data.target)\n",
    "all = [(len(post[0]), post[0], train_data.target_names[post[1]])\n",
    "       for post in post_group]\n",
    "graphics = sorted([post for post in all if \n",
    "                  post[2]=='comp.graphics'])\n",
    "print(graphics[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['situnaya', 'ibm3090', 'bham', 'ac', 'uk', 'subject', 'test', 'sorri', 'organ', 'univers', 'birmingham', 'unit', 'kingdom', 'line', 'nntp', 'post', 'host', 'ibm3090', 'bham', 'ac', 'uk']\n"
     ]
    }
   ],
   "source": [
    "noise_post = graphics[5][1]\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "print(list(analyzer(noise_post)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ac', 'birmingham', 'host', 'kingdom', 'nntp', 'sorri', 'test', 'uk', 'unit', 'univers']\n"
     ]
    }
   ],
   "source": [
    "useful = set(analyzer(noise_post)).intersection(\n",
    "        vectorizer.get_feature_names())\n",
    "print(sorted(useful))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF(ac)=3.51\n",
      "IDF(birmingham)=6.77\n",
      "IDF(host)=1.74\n",
      "IDF(kingdom)=6.68\n",
      "IDF(nntp)=1.77\n",
      "IDF(sorri)=4.14\n",
      "IDF(test)=3.83\n",
      "IDF(uk)=3.70\n",
      "IDF(unit)=4.42\n",
      "IDF(univers)=1.91\n"
     ]
    }
   ],
   "source": [
    "#notice that all of these idf scores are pretty low.\n",
    "#also, that the highest rated have nothing to do with \n",
    "#computer graphics\n",
    "for term in sorted(useful):\n",
    "    print('IDF(%s)=%.2f'%(term,\n",
    "            vectorizer._tfidf.idf_[vectorizer.vocabulary_[term]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Further Reading:\n",
    "* Python 3 Text Processing with NLTK 3 Cookbook\n",
    "* [sklearn clustering docs](http://scikit-learn.org/dev/modules/clustering.html)\n",
    "* sklearn metrics docs - to learn more about measuring kmeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
